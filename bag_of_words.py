# -*- coding: utf-8 -*-
"""Bag of Words.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kyEj_ZxptmxRnvaQTK-bWw0R7QjQ3sZs
"""

import nltk

paragraph = """At SpaceX, Musk oversees the development of rockets and spacecraft for missions to Earth orbit and ultimately to other planets. In 2008, SpaceXâ€™s Falcon 9 rocket and Dragon spacecraft won the NASA contract to provide cargo transport to space. In 2012, SpaceX became the first commercial company to dock with the International Space Station and return cargo to Earth with the Dragon.

At Tesla, Musk has overseen product development and design from the beginning, including the all-electric Tesla Roadster, Model S and Model X, and the rollout of Supercharger stations to keep the cars juiced up. (Some of the charging stations use solar energy systems from SolarCity, of which Musk is the non-executive chair.) Transitioning to a sustainable energy economy, in which electric vehicles play a pivotal role, has been one of his central interests for almost two decades. Before this, he co-founded PayPal and served as the company's chair and CEO."""

#cleaning the texts
import re
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

ps = PorterStemmer()
lem = WordNetLemmatizer()

nltk.download('punkt')

sentences = nltk.sent_tokenize(paragraph)

nltk.download('stopwords')

#cleaning by using the stemming
new_sent = [] #new sentences will be appended to this emptylist
for i in range(len(sentences)):
  cleaning = re.sub('[^a-zA-z]',' ',sentences[i]) #removing the ,.'/ these symbols from the sentences by using the regular expressions
  cleaning = cleaning.lower() # making the small letters
  cleaning = cleaning.split() #split functions makes the every sentence will be splitted the return value is a list
  cleaning = [ps.stem(word) for word in cleaning if word not in set(stopwords.words('english'))] #aplling the stemming and removing the stop words like:  is, he, are etc; 
  cleaning = " ".join(cleaning) #again combining the words to form into sentences
  new_sent.append(cleaning) #appending into the empty list

new_sent

nltk.download('wordnet')

# cleaning by lemmatiation
new_sent_lem = []
for i in range(len(sentences)):
  clean_lem = re.sub('[^a-zA-z]',' ',sentences[i])
  clean_lem = clean_lem.lower()
  clean_lem = clean_lem.split()
  clean_lem = [lem.lemmatize(word) for word in clean_lem if word not in set(stopwords.words('english'))]
  clean_lem = " ".join(clean_lem)
  new_sent_lem.append(clean_lem)

new_sent_lem

#creating the bag of words model
#appliying the leammatization words to the bag of words model
from sklearn.feature_extraction.text import CountVectorizer #countvectorizer responsible for the making the histogram and sorting in a decending order  and matching the features
final_matrix = CountVectorizer().fit_transform(new_sent_lem).toarray() #fit transform is responsible for the below matrix creation

final_matrix

#bag of words model for stemming words but using the lemmatization is preferable for the sentimental analysis 
from sklearn.feature_extraction.text import CountVectorizer
final_mat_stem = CountVectorizer().fit_transform(new_sent).toarray()

final_mat_stem